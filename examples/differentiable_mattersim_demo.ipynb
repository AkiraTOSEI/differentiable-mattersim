{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 微分可能な MatterSim のデモ\n",
    "\n",
    "このノートブックでは、MatterSim を PyTorch の autograd と統合し、入力（原子種、格子、座標）に対して勾配を計算する方法を示します。\n",
    "\n",
    "## 概要\n",
    "\n",
    "- **目的**: MatterSim を損失関数の一部として使用し、入力を最適化する\n",
    "- **新しい API**: `DifferentiableMatterSimCalculator`\n",
    "- **主な機能**: 原子種・格子・座標への勾配計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mattersim.forcefield.differentiable_potential'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/fujii/differentiable_mattersim/examples/differentiable_mattersim_demo.ipynb セル 2\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsonata/home/fujii/differentiable_mattersim/examples/differentiable_mattersim_demo.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mappend(\u001b[39m'\u001b[39m\u001b[39m../../\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# Adjust the path as needed to locate mattersim\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsonata/home/fujii/differentiable_mattersim/examples/differentiable_mattersim_demo.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mmattersim\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsonata/home/fujii/differentiable_mattersim/examples/differentiable_mattersim_demo.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mmattersim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mforcefield\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdifferentiable_potential\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m DifferentiableMatterSimCalculator\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsonata/home/fujii/differentiable_mattersim/examples/differentiable_mattersim_demo.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPyTorch version: \u001b[39m\u001b[39m{\u001b[39;00mtorch\u001b[39m.\u001b[39m__version__\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsonata/home/fujii/differentiable_mattersim/examples/differentiable_mattersim_demo.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDevice: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39mif\u001b[39;00m\u001b[39m \u001b[39mtorch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available()\u001b[39m \u001b[39m\u001b[39melse\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mattersim.forcefield.differentiable_potential'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ase.build import bulk\n",
    "import sys\n",
    "\n",
    "sys.path.append('../../')  # Adjust the path as needed to locate mattersim\n",
    "import mattersim\n",
    "from mattersim.forcefield.differentiable_potential import DifferentiableMatterSimCalculator\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 基本的な使い方：勾配の計算\n",
    "\n",
    "まず、Diamond Si 構造を作成し、座標に対する勾配を計算します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diamond Si 構造を作成\n",
    "si = bulk(\"Si\", \"diamond\", a=5.43)\n",
    "print(f\"原子数: {len(si)}\")\n",
    "print(f\"座標:\\n{si.get_positions()}\")\n",
    "print(f\"格子:\\n{si.cell.array}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 微分可能な Calculator を初期化\n",
    "device = \"cpu\"  # または \"cuda\"\n",
    "calc = DifferentiableMatterSimCalculator(device=device)\n",
    "print(\"Calculator が初期化されました\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 座標を tensor として取得し、requires_grad を設定\n",
    "positions = torch.tensor(\n",
    "    si.get_positions(), dtype=torch.float32, device=device, requires_grad=True\n",
    ")\n",
    "\n",
    "print(f\"座標 tensor の形状: {positions.shape}\")\n",
    "print(f\"requires_grad: {positions.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エネルギーを計算\n",
    "output = calc.forward(si, positions=positions, include_forces=False)\n",
    "energy = output[\"total_energy\"]\n",
    "\n",
    "print(f\"エネルギー: {energy.item():.6f} eV\")\n",
    "print(f\"エネルギー tensor の形状: {energy.shape}\")\n",
    "print(f\"requires_grad: {energy.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward で勾配を計算\n",
    "energy.backward()\n",
    "\n",
    "print(f\"座標の勾配形状: {positions.grad.shape}\")\n",
    "print(f\"座標の勾配:\\n{positions.grad}\")\n",
    "print(f\"勾配のノルム: {torch.norm(positions.grad).item():.6e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ **結果**: 座標に対する勾配が正常に計算されました！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 構造最適化\n",
    "\n",
    "ランダムに摂動を加えた構造を、PyTorch の optimizer で最適化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新しい構造を作成（摂動を加える）\n",
    "si_perturbed = bulk(\"Si\", \"diamond\", a=5.43)\n",
    "positions_opt = torch.tensor(si_perturbed.get_positions(), dtype=torch.float32, device=device)\n",
    "\n",
    "# ランダムに摂動\n",
    "torch.manual_seed(42)  # 再現性のため\n",
    "perturbation = torch.randn_like(positions_opt) * 0.05  # 0.05 Å の摂動\n",
    "positions_opt = positions_opt + perturbation\n",
    "positions_opt.requires_grad_(True)\n",
    "\n",
    "print(f\"摂動のノルム: {torch.norm(perturbation).item():.6f} Å\")\n",
    "print(f\"摂動後の座標:\\n{positions_opt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculator を初期化（構造が変わったので再初期化）\n",
    "calc_opt = DifferentiableMatterSimCalculator(device=device)\n",
    "\n",
    "# Adam オプティマイザ\n",
    "optimizer = torch.optim.Adam([positions_opt], lr=0.02)\n",
    "\n",
    "# 最適化の履歴を記録\n",
    "energy_history = []\n",
    "grad_norm_history = []\n",
    "\n",
    "print(\"最適化を開始します...\")\n",
    "print(\"ステップ | エネルギー (eV) | 勾配ノルム\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "num_steps = 30\n",
    "for step in range(num_steps):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # エネルギーを計算\n",
    "    output = calc_opt.forward(si_perturbed, positions=positions_opt, include_forces=False)\n",
    "    energy = output[\"total_energy\"]\n",
    "    \n",
    "    # backward\n",
    "    energy.backward()\n",
    "    \n",
    "    # 履歴を記録\n",
    "    energy_history.append(energy.item())\n",
    "    grad_norm = torch.norm(positions_opt.grad).item()\n",
    "    grad_norm_history.append(grad_norm)\n",
    "    \n",
    "    # 最適化ステップ\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 5 == 0 or step == num_steps - 1:\n",
    "        print(f\"{step:7d} | {energy.item():14.6f} | {grad_norm:.6e}\")\n",
    "\n",
    "print(f\"\\n初期エネルギー: {energy_history[0]:.6f} eV\")\n",
    "print(f\"最終エネルギー: {energy_history[-1]:.6f} eV\")\n",
    "print(f\"エネルギー減少: {energy_history[0] - energy_history[-1]:.6f} eV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適化の履歴をプロット\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# エネルギーの推移\n",
    "ax1.plot(energy_history, 'b-', linewidth=2)\n",
    "ax1.set_xlabel('最適化ステップ', fontsize=12)\n",
    "ax1.set_ylabel('エネルギー (eV)', fontsize=12)\n",
    "ax1.set_title('エネルギーの推移', fontsize=14)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 勾配ノルムの推移\n",
    "ax2.semilogy(grad_norm_history, 'r-', linewidth=2)\n",
    "ax2.set_xlabel('最適化ステップ', fontsize=12)\n",
    "ax2.set_ylabel('勾配ノルム', fontsize=12)\n",
    "ax2.set_title('勾配ノルムの推移', fontsize=14)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ **結果**: エネルギーが減少し、最適化が正常に動作しました！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 格子定数の最適化\n",
    "\n",
    "格子定数を少しずらした構造から、正しい格子定数を最適化で求めます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 格子定数を少しずらした構造\n",
    "a_initial = 5.5  # 正しい値は 5.43\n",
    "si_lattice = bulk(\"Si\", \"diamond\", a=a_initial)\n",
    "\n",
    "print(f\"初期格子定数: a = {a_initial:.3f} Å\")\n",
    "print(f\"正解の格子定数: a = 5.43 Å\")\n",
    "print(f\"初期格子行列:\\n{si_lattice.cell.array}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculator を初期化\n",
    "calc_lattice = DifferentiableMatterSimCalculator(device=device)\n",
    "\n",
    "# 格子を tensor として取得\n",
    "lattice_opt = torch.tensor(si_lattice.cell.array, dtype=torch.float32, device=device)\n",
    "lattice_opt.requires_grad_(True)\n",
    "\n",
    "# SGD オプティマイザ（格子は学習率を小さくする）\n",
    "optimizer_lattice = torch.optim.SGD([lattice_opt], lr=0.01)\n",
    "\n",
    "# 最適化の履歴\n",
    "energy_lattice_history = []\n",
    "lattice_a_history = []\n",
    "\n",
    "print(\"格子定数の最適化を開始します...\")\n",
    "print(\"ステップ | エネルギー (eV) | 格子定数 a (Å)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "num_steps_lattice = 40\n",
    "for step in range(num_steps_lattice):\n",
    "    optimizer_lattice.zero_grad()\n",
    "    \n",
    "    # エネルギーを計算\n",
    "    output = calc_lattice.forward(si_lattice, lattice=lattice_opt, include_forces=False)\n",
    "    energy = output[\"total_energy\"]\n",
    "    \n",
    "    # backward\n",
    "    energy.backward()\n",
    "    \n",
    "    # 最適化ステップ\n",
    "    optimizer_lattice.step()\n",
    "    \n",
    "    # 現在の格子定数を記録\n",
    "    current_a = lattice_opt[0, 0].item()\n",
    "    energy_lattice_history.append(energy.item())\n",
    "    lattice_a_history.append(current_a)\n",
    "    \n",
    "    if step % 5 == 0 or step == num_steps_lattice - 1:\n",
    "        print(f\"{step:7d} | {energy.item():14.6f} | {current_a:.6f}\")\n",
    "\n",
    "final_a = lattice_opt[0, 0].item()\n",
    "print(f\"\\n初期格子定数: a = {a_initial:.6f} Å\")\n",
    "print(f\"最終格子定数: a = {final_a:.6f} Å\")\n",
    "print(f\"正解値: a = 5.43 Å\")\n",
    "print(f\"誤差: {abs(final_a - 5.43):.6f} Å\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 格子最適化の履歴をプロット\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# エネルギーの推移\n",
    "ax1.plot(energy_lattice_history, 'b-', linewidth=2)\n",
    "ax1.set_xlabel('最適化ステップ', fontsize=12)\n",
    "ax1.set_ylabel('エネルギー (eV)', fontsize=12)\n",
    "ax1.set_title('エネルギーの推移（格子最適化）', fontsize=14)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 格子定数の推移\n",
    "ax2.plot(lattice_a_history, 'g-', linewidth=2, label='最適化された値')\n",
    "ax2.axhline(y=5.43, color='r', linestyle='--', linewidth=2, label='正解値')\n",
    "ax2.set_xlabel('最適化ステップ', fontsize=12)\n",
    "ax2.set_ylabel('格子定数 a (Å)', fontsize=12)\n",
    "ax2.set_title('格子定数の推移', fontsize=14)\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 複数パラメータの同時最適化\n",
    "\n",
    "座標と格子を同時に最適化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新しい構造（座標と格子の両方を摂動）\n",
    "si_joint = bulk(\"Si\", \"diamond\", a=5.5)\n",
    "\n",
    "# 座標を摂動\n",
    "positions_joint = torch.tensor(si_joint.get_positions(), dtype=torch.float32, device=device)\n",
    "torch.manual_seed(123)\n",
    "positions_joint = positions_joint + torch.randn_like(positions_joint) * 0.03\n",
    "positions_joint.requires_grad_(True)\n",
    "\n",
    "# 格子も tensor に\n",
    "lattice_joint = torch.tensor(si_joint.cell.array, dtype=torch.float32, device=device)\n",
    "lattice_joint.requires_grad_(True)\n",
    "\n",
    "print(f\"初期格子定数: a = {lattice_joint[0, 0].item():.6f} Å\")\n",
    "print(f\"座標の摂動ノルム: {torch.norm(positions_joint - torch.tensor(si_joint.get_positions(), device=device)).item():.6f} Å\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculator を初期化\n",
    "calc_joint = DifferentiableMatterSimCalculator(device=device)\n",
    "\n",
    "# オプティマイザ（両方のパラメータを含む）\n",
    "optimizer_joint = torch.optim.Adam([\n",
    "    {'params': [positions_joint], 'lr': 0.02},\n",
    "    {'params': [lattice_joint], 'lr': 0.005},  # 格子は学習率を小さく\n",
    "])\n",
    "\n",
    "# 最適化の履歴\n",
    "energy_joint_history = []\n",
    "lattice_a_joint_history = []\n",
    "\n",
    "print(\"同時最適化を開始します...\")\n",
    "print(\"ステップ | エネルギー (eV) | 格子定数 a (Å)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "num_steps_joint = 30\n",
    "for step in range(num_steps_joint):\n",
    "    optimizer_joint.zero_grad()\n",
    "    \n",
    "    # エネルギーを計算\n",
    "    output = calc_joint.forward(\n",
    "        si_joint, \n",
    "        positions=positions_joint, \n",
    "        lattice=lattice_joint, \n",
    "        include_forces=False\n",
    "    )\n",
    "    energy = output[\"total_energy\"]\n",
    "    \n",
    "    # backward\n",
    "    energy.backward()\n",
    "    \n",
    "    # 最適化ステップ\n",
    "    optimizer_joint.step()\n",
    "    \n",
    "    # 履歴を記録\n",
    "    current_a = lattice_joint[0, 0].item()\n",
    "    energy_joint_history.append(energy.item())\n",
    "    lattice_a_joint_history.append(current_a)\n",
    "    \n",
    "    if step % 5 == 0 or step == num_steps_joint - 1:\n",
    "        print(f\"{step:7d} | {energy.item():14.6f} | {current_a:.6f}\")\n",
    "\n",
    "print(f\"\\n最終エネルギー: {energy_joint_history[-1]:.6f} eV\")\n",
    "print(f\"最終格子定数: a = {lattice_a_joint_history[-1]:.6f} Å\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 同時最適化の履歴をプロット\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "# 左軸: エネルギー\n",
    "ax.plot(energy_joint_history, 'b-', linewidth=2, label='エネルギー')\n",
    "ax.set_xlabel('最適化ステップ', fontsize=12)\n",
    "ax.set_ylabel('エネルギー (eV)', fontsize=12, color='b')\n",
    "ax.tick_params(axis='y', labelcolor='b')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 右軸: 格子定数\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(lattice_a_joint_history, 'g-', linewidth=2, label='格子定数 a')\n",
    "ax2.axhline(y=5.43, color='r', linestyle='--', linewidth=2, label='正解値')\n",
    "ax2.set_ylabel('格子定数 a (Å)', fontsize=12, color='g')\n",
    "ax2.tick_params(axis='y', labelcolor='g')\n",
    "\n",
    "# タイトルと凡例\n",
    "ax.set_title('座標と格子の同時最適化', fontsize=14)\n",
    "lines1, labels1 = ax.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax.legend(lines1 + lines2, labels1 + labels2, loc='upper right', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ **結果**: 座標と格子の両方が同時に最適化されました！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. まとめ\n",
    "\n",
    "このノートブックでは、以下を実演しました：\n",
    "\n",
    "1. ✅ **勾配の計算**: 座標に対する勾配を backward() で計算\n",
    "2. ✅ **構造最適化**: ランダムに摂動した構造を Adam で最適化\n",
    "3. ✅ **格子最適化**: 格子定数を SGD で最適化\n",
    "4. ✅ **同時最適化**: 座標と格子を同時に最適化\n",
    "\n",
    "### 主なポイント\n",
    "\n",
    "- `DifferentiableMatterSimCalculator` を使用\n",
    "- 入力 tensor に `requires_grad=True` を設定\n",
    "- `energy.backward()` で勾配を計算\n",
    "- PyTorch の optimizer で入力を更新\n",
    "\n",
    "### 次のステップ\n",
    "\n",
    "- より複雑な損失関数を定義（例: 目標構造との距離）\n",
    "- 複数構造のバッチ処理\n",
    "- カスタム最適化アルゴリズムの実装\n",
    "\n",
    "詳細なドキュメントは `docs/DIFFERENTIABLE_API.md` を参照してください。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
